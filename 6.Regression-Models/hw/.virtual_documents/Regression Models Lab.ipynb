%matplotlib inline


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import mean_squared_error, r2_score











np.random.seed(37)

n_samples = 300  
m_features = 10 

# generate random feature data with different ranges
X = np.random.rand(n_samples, m_features) * np.random.randint(1, 100, size = (1, m_features))

a = np.random.uniform(-10, 10, m_features) 
b = np.random.uniform(-20, 20)

# y is calculated as a linear combination of the features plus intercept
y = X @ a + b

noise = np.random.normal(loc = 0, scale = 5, size = y.shape)
y += noise


df = pd.DataFrame(X, columns=[f"x{i+1}" for i in range(m_features)])
df["y"] = y
df.to_csv("linear_regr_data.csv", index = False)





generated_data = pd.read_csv("linear_regr_data.csv")


generated_data.head()


generated_data.shape


generated_data.describe()





correlation_matrix = df.corr()
correlation_matrix


# check the correlation between features and 'y'
print(correlation_matrix.y)





X = df.drop("y", axis = 1)
y = df["y"]


model = LinearRegression()
model.fit(X, y)
y_pred = model.predict(X)

mse = mean_squared_error(y, y_pred)
r2 = r2_score(y, y_pred)

print(f"Mean Squared Error = {mse}")
print(f"R^2 = {r2}")

print(f"coefficients = {model.coef_}")
print(f"intercept = {model.intercept_}")








# add a column of '1's to X to include the intercept
X_with_intercept = np.hstack((X, np.ones((X.shape[0], 1))))

model = LinearRegression()
model.fit(X_with_intercept, y)

coefficients = model.coef_
intercept = model.intercept_

print(f"Coefficients including intercept: {coefficients}")
print(f"Intercept: {intercept}")





def model_function(coefficients, X):
    X_with_intercept = np.hstack((X, np.ones((X.shape[0], 1))))
    y_pred = np.dot(X_with_intercept, coefficients)
    
    return y_pred
    
y_pred = model_function(coefficients, X)
print(y_pred)
print(f"Mean Squared Error: {mean_squared_error(y, y_pred)}")
print(f"R^2 = {r2_score(y, y_pred)}")





# calculates the mean squared error (MSE) between the actual values ​​and the predicted values:
def compute_cost(X, y, coefficients, intercept):
    predictions = X @ coefficients + intercept
    mse = np.mean((y - predictions) ** 2)
    
    return mse


# calculates the gradients of the cost function about the coefficients and the intercept:
def compute_gradients(X, y, coefficients, intercept):
    predictions = X @ coefficients + intercept
    gradients = -2 / len(y) * (X.T @ (y - predictions))
    intercept_gradient = -2 / len(y) * np.sum(y - predictions)
    
    return np.append(gradients, intercept_gradient)


# use generated data from previous tasks:
df = pd.read_csv("linear_regr_data.csv")
X = df.drop("y", axis = 1).values
y = df.y.values

model = LinearRegression()
model.fit(X, y)

coefficients = model.coef_
intercept = model.intercept_

mse = compute_cost(X, y, coefficients, intercept)
gradients = compute_gradients(X, y, coefficients, intercept)

print(f"MSE = {mse}")
print(f"Gradients including intercept: {gradients}")




































