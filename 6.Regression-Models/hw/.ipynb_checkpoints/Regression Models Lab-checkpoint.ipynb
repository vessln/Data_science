{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a21845-5edc-410a-ab41-5196319ddec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11375a92-fad2-4431-a3e4-bc527298d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3786fdb-7af6-4ce2-b730-90c60a5d896f",
   "metadata": {},
   "source": [
    "# Regression Models Lab\n",
    "## Linear and logistic regression: theory and practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d6ff4b-e18a-4580-8c7c-54f4c7ef2dc9",
   "metadata": {},
   "source": [
    "In this lab you'll revisit and expand on your knowledge of modelling in general, as well as the fundamentals of linear and logistic regression. As a reminder, _linear regression_ is a regression model (regressor), and _logistic regression_ is a classification model (classifier).\n",
    "\n",
    "This time, you'll use generated data, in order to separate some of the complexity of handling various datasets from inspecting and evaluating models.\n",
    "\n",
    "**Use vectorization as much as possible!** You should be able to complete the lab using for-loops only to track the training steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8a9603-c803-4728-a69d-b6acfe2bad8a",
   "metadata": {},
   "source": [
    "### Problem 1. Generate some data for multiple linear regression (1 point)\n",
    "As an expansion to the lecture, you'll create a dataset and a model.\n",
    "\n",
    "Create a dataset of some (e.g., 50-500) observations of several (e.g., 5-20) independent features. You can use random generators for them; think about what distributions you'd like to use. Let's call them $x_1, x_2, ..., x_m$. The data matrix $X$ you should get should be of size $n \\times m$. It's best if all features have different ranges.\n",
    "\n",
    "Create the dependent variable by assigning coefficients $\\bar{a_1}, \\bar{a_2}, ..., \\bar{a_m}, \\bar{b}$ and calculating $y$ as a linear combination of the input features. Add some random noise to the functional values. I've used bars over coefficients to avoid confusion with the model parameters later.\n",
    "\n",
    "Save the dataset ($X$ and $y$), and \"forget\" that the coefficients have ever existed. \"All\" you have is the file and the implicit assumption that there is a linear relationship between $X$ and $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7d5f0fe-d21e-47cc-b792-599740c4ad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(37)\n",
    "\n",
    "n_samples = 300  \n",
    "m_features = 10 \n",
    "\n",
    "# generate random feature data with different ranges\n",
    "X = np.random.rand(n_samples, m_features) * np.random.randint(1, 100, size = (1, m_features))\n",
    "\n",
    "a = np.random.uniform(-10, 10, m_features) \n",
    "b = np.random.uniform(-20, 20)\n",
    "\n",
    "# y is calculated as a linear combination of the features plus intercept\n",
    "y = X @ a + b\n",
    "\n",
    "noise = np.random.normal(loc = 0, scale = 5, size = y.shape)\n",
    "y += noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a794fb-7101-4dad-9899-1bd1bf3260ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X, columns=[f\"x{i+1}\" for i in range(m_features)])\n",
    "df[\"y\"] = y\n",
    "df.to_csv(\"linear_regr_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bb4164-eaa3-4b76-ae3f-36da95416b7b",
   "metadata": {},
   "source": [
    "### Problem 2. Check your assumption (1 point)\n",
    "Read the dataset you just saved (this is just to simulate starting a new project). It's a good idea to test and verify our assumptions. Find a way to check whether there really is a linear relationship between the features and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d13c8452-5eb1-4719-88da-372331cfce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = pd.read_csv(\"linear_regr_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96fb9d49-69b8-4999-ad5d-847761b0a311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.057381</td>\n",
       "      <td>20.884418</td>\n",
       "      <td>2.891925</td>\n",
       "      <td>13.383582</td>\n",
       "      <td>6.820926</td>\n",
       "      <td>11.631808</td>\n",
       "      <td>5.999378</td>\n",
       "      <td>49.946876</td>\n",
       "      <td>18.046660</td>\n",
       "      <td>6.780821</td>\n",
       "      <td>672.259958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.744697</td>\n",
       "      <td>28.233027</td>\n",
       "      <td>6.652333</td>\n",
       "      <td>22.159703</td>\n",
       "      <td>9.867732</td>\n",
       "      <td>3.345673</td>\n",
       "      <td>34.565166</td>\n",
       "      <td>38.416255</td>\n",
       "      <td>45.116759</td>\n",
       "      <td>8.363358</td>\n",
       "      <td>458.292144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.793044</td>\n",
       "      <td>6.066540</td>\n",
       "      <td>1.755428</td>\n",
       "      <td>8.388669</td>\n",
       "      <td>8.837519</td>\n",
       "      <td>16.494126</td>\n",
       "      <td>19.312797</td>\n",
       "      <td>32.054132</td>\n",
       "      <td>11.451997</td>\n",
       "      <td>8.122810</td>\n",
       "      <td>198.403153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.725149</td>\n",
       "      <td>8.948724</td>\n",
       "      <td>5.485293</td>\n",
       "      <td>13.698392</td>\n",
       "      <td>0.441037</td>\n",
       "      <td>14.018676</td>\n",
       "      <td>37.135740</td>\n",
       "      <td>1.282180</td>\n",
       "      <td>51.586456</td>\n",
       "      <td>3.644034</td>\n",
       "      <td>17.293492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.126136</td>\n",
       "      <td>20.935504</td>\n",
       "      <td>5.052764</td>\n",
       "      <td>12.314186</td>\n",
       "      <td>10.304708</td>\n",
       "      <td>14.901429</td>\n",
       "      <td>43.620313</td>\n",
       "      <td>31.154744</td>\n",
       "      <td>45.049642</td>\n",
       "      <td>0.348492</td>\n",
       "      <td>123.786151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x1         x2        x3         x4         x5         x6         x7  \\\n",
       "0  33.057381  20.884418  2.891925  13.383582   6.820926  11.631808   5.999378   \n",
       "1  27.744697  28.233027  6.652333  22.159703   9.867732   3.345673  34.565166   \n",
       "2   1.793044   6.066540  1.755428   8.388669   8.837519  16.494126  19.312797   \n",
       "3  26.725149   8.948724  5.485293  13.698392   0.441037  14.018676  37.135740   \n",
       "4   0.126136  20.935504  5.052764  12.314186  10.304708  14.901429  43.620313   \n",
       "\n",
       "          x8         x9       x10           y  \n",
       "0  49.946876  18.046660  6.780821  672.259958  \n",
       "1  38.416255  45.116759  8.363358  458.292144  \n",
       "2  32.054132  11.451997  8.122810  198.403153  \n",
       "3   1.282180  51.586456  3.644034   17.293492  \n",
       "4  31.154744  45.049642  0.348492  123.786151  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8795faa-a6ed-49ad-8208-455a488eaf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 11)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2196bfca-42a0-4b5d-a631-1ab054f34b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16.830681</td>\n",
       "      <td>22.361267</td>\n",
       "      <td>7.673649</td>\n",
       "      <td>11.503659</td>\n",
       "      <td>5.336740</td>\n",
       "      <td>8.844550</td>\n",
       "      <td>28.618739</td>\n",
       "      <td>32.518246</td>\n",
       "      <td>33.124964</td>\n",
       "      <td>4.518033</td>\n",
       "      <td>337.478300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.968977</td>\n",
       "      <td>13.313601</td>\n",
       "      <td>4.449431</td>\n",
       "      <td>6.954206</td>\n",
       "      <td>3.149898</td>\n",
       "      <td>5.068512</td>\n",
       "      <td>15.911509</td>\n",
       "      <td>19.545879</td>\n",
       "      <td>19.136938</td>\n",
       "      <td>2.468227</td>\n",
       "      <td>189.173150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.023233</td>\n",
       "      <td>0.215109</td>\n",
       "      <td>0.091996</td>\n",
       "      <td>0.019503</td>\n",
       "      <td>0.003120</td>\n",
       "      <td>0.035669</td>\n",
       "      <td>1.003963</td>\n",
       "      <td>0.146881</td>\n",
       "      <td>0.166794</td>\n",
       "      <td>0.044184</td>\n",
       "      <td>-168.224658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.567395</td>\n",
       "      <td>9.531301</td>\n",
       "      <td>4.181432</td>\n",
       "      <td>5.049010</td>\n",
       "      <td>2.720554</td>\n",
       "      <td>4.406973</td>\n",
       "      <td>14.035982</td>\n",
       "      <td>15.212299</td>\n",
       "      <td>15.830065</td>\n",
       "      <td>2.497605</td>\n",
       "      <td>223.326088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16.431257</td>\n",
       "      <td>21.955199</td>\n",
       "      <td>7.354771</td>\n",
       "      <td>11.620573</td>\n",
       "      <td>5.212516</td>\n",
       "      <td>8.706587</td>\n",
       "      <td>29.504331</td>\n",
       "      <td>32.174301</td>\n",
       "      <td>33.934953</td>\n",
       "      <td>4.400607</td>\n",
       "      <td>342.694485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>24.540754</td>\n",
       "      <td>33.677593</td>\n",
       "      <td>11.775577</td>\n",
       "      <td>17.349510</td>\n",
       "      <td>8.004582</td>\n",
       "      <td>13.475807</td>\n",
       "      <td>41.154500</td>\n",
       "      <td>48.392262</td>\n",
       "      <td>50.137675</td>\n",
       "      <td>6.491246</td>\n",
       "      <td>449.524146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>34.873387</td>\n",
       "      <td>44.965178</td>\n",
       "      <td>14.994918</td>\n",
       "      <td>22.965378</td>\n",
       "      <td>10.955257</td>\n",
       "      <td>16.978691</td>\n",
       "      <td>57.747651</td>\n",
       "      <td>66.754757</td>\n",
       "      <td>63.950567</td>\n",
       "      <td>8.907326</td>\n",
       "      <td>865.653912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               x1          x2          x3          x4          x5          x6  \\\n",
       "count  300.000000  300.000000  300.000000  300.000000  300.000000  300.000000   \n",
       "mean    16.830681   22.361267    7.673649   11.503659    5.336740    8.844550   \n",
       "std      9.968977   13.313601    4.449431    6.954206    3.149898    5.068512   \n",
       "min      0.023233    0.215109    0.091996    0.019503    0.003120    0.035669   \n",
       "25%      8.567395    9.531301    4.181432    5.049010    2.720554    4.406973   \n",
       "50%     16.431257   21.955199    7.354771   11.620573    5.212516    8.706587   \n",
       "75%     24.540754   33.677593   11.775577   17.349510    8.004582   13.475807   \n",
       "max     34.873387   44.965178   14.994918   22.965378   10.955257   16.978691   \n",
       "\n",
       "               x7          x8          x9         x10           y  \n",
       "count  300.000000  300.000000  300.000000  300.000000  300.000000  \n",
       "mean    28.618739   32.518246   33.124964    4.518033  337.478300  \n",
       "std     15.911509   19.545879   19.136938    2.468227  189.173150  \n",
       "min      1.003963    0.146881    0.166794    0.044184 -168.224658  \n",
       "25%     14.035982   15.212299   15.830065    2.497605  223.326088  \n",
       "50%     29.504331   32.174301   33.934953    4.400607  342.694485  \n",
       "75%     41.154500   48.392262   50.137675    6.491246  449.524146  \n",
       "max     57.747651   66.754757   63.950567    8.907326  865.653912  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7989f6ad-7787-4d35-b04a-0cb799817938",
   "metadata": {},
   "source": [
    "To check whether there is a linear relationship between the features and output I make the correlation matrix. It shows how strongly each feature is related to the target variable y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7109ba3-cb94-4751-8c1c-cffe0567e163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.039936</td>\n",
       "      <td>0.003526</td>\n",
       "      <td>0.007280</td>\n",
       "      <td>-0.007541</td>\n",
       "      <td>-0.021245</td>\n",
       "      <td>0.022977</td>\n",
       "      <td>0.179157</td>\n",
       "      <td>0.126103</td>\n",
       "      <td>-0.013969</td>\n",
       "      <td>0.424590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x2</th>\n",
       "      <td>-0.039936</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002927</td>\n",
       "      <td>0.015172</td>\n",
       "      <td>0.032477</td>\n",
       "      <td>0.050837</td>\n",
       "      <td>0.106724</td>\n",
       "      <td>-0.075864</td>\n",
       "      <td>-0.042676</td>\n",
       "      <td>-0.093224</td>\n",
       "      <td>0.446384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x3</th>\n",
       "      <td>0.003526</td>\n",
       "      <td>-0.002927</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017427</td>\n",
       "      <td>0.012066</td>\n",
       "      <td>0.091463</td>\n",
       "      <td>-0.083206</td>\n",
       "      <td>-0.024772</td>\n",
       "      <td>-0.038316</td>\n",
       "      <td>-0.018879</td>\n",
       "      <td>0.139539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x4</th>\n",
       "      <td>0.007280</td>\n",
       "      <td>0.015172</td>\n",
       "      <td>0.017427</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018564</td>\n",
       "      <td>-0.114997</td>\n",
       "      <td>-0.017722</td>\n",
       "      <td>0.021163</td>\n",
       "      <td>0.031964</td>\n",
       "      <td>0.035033</td>\n",
       "      <td>-0.006333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x5</th>\n",
       "      <td>-0.007541</td>\n",
       "      <td>0.032477</td>\n",
       "      <td>0.012066</td>\n",
       "      <td>0.018564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.063668</td>\n",
       "      <td>0.051840</td>\n",
       "      <td>0.095510</td>\n",
       "      <td>-0.004134</td>\n",
       "      <td>-0.011656</td>\n",
       "      <td>0.163366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x6</th>\n",
       "      <td>-0.021245</td>\n",
       "      <td>0.050837</td>\n",
       "      <td>0.091463</td>\n",
       "      <td>-0.114997</td>\n",
       "      <td>0.063668</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.007378</td>\n",
       "      <td>-0.056266</td>\n",
       "      <td>0.100071</td>\n",
       "      <td>0.024867</td>\n",
       "      <td>0.011195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x7</th>\n",
       "      <td>0.022977</td>\n",
       "      <td>0.106724</td>\n",
       "      <td>-0.083206</td>\n",
       "      <td>-0.017722</td>\n",
       "      <td>0.051840</td>\n",
       "      <td>-0.007378</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.024446</td>\n",
       "      <td>0.095141</td>\n",
       "      <td>-0.001231</td>\n",
       "      <td>-0.446722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x8</th>\n",
       "      <td>0.179157</td>\n",
       "      <td>-0.075864</td>\n",
       "      <td>-0.024772</td>\n",
       "      <td>0.021163</td>\n",
       "      <td>0.095510</td>\n",
       "      <td>-0.056266</td>\n",
       "      <td>-0.024446</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.027991</td>\n",
       "      <td>0.028240</td>\n",
       "      <td>0.592573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x9</th>\n",
       "      <td>0.126103</td>\n",
       "      <td>-0.042676</td>\n",
       "      <td>-0.038316</td>\n",
       "      <td>0.031964</td>\n",
       "      <td>-0.004134</td>\n",
       "      <td>0.100071</td>\n",
       "      <td>0.095141</td>\n",
       "      <td>-0.027991</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111288</td>\n",
       "      <td>-0.169282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x10</th>\n",
       "      <td>-0.013969</td>\n",
       "      <td>-0.093224</td>\n",
       "      <td>-0.018879</td>\n",
       "      <td>0.035033</td>\n",
       "      <td>-0.011656</td>\n",
       "      <td>0.024867</td>\n",
       "      <td>-0.001231</td>\n",
       "      <td>0.028240</td>\n",
       "      <td>0.111288</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.050284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0.424590</td>\n",
       "      <td>0.446384</td>\n",
       "      <td>0.139539</td>\n",
       "      <td>-0.006333</td>\n",
       "      <td>0.163366</td>\n",
       "      <td>0.011195</td>\n",
       "      <td>-0.446722</td>\n",
       "      <td>0.592573</td>\n",
       "      <td>-0.169282</td>\n",
       "      <td>-0.050284</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1        x2        x3        x4        x5        x6        x7  \\\n",
       "x1   1.000000 -0.039936  0.003526  0.007280 -0.007541 -0.021245  0.022977   \n",
       "x2  -0.039936  1.000000 -0.002927  0.015172  0.032477  0.050837  0.106724   \n",
       "x3   0.003526 -0.002927  1.000000  0.017427  0.012066  0.091463 -0.083206   \n",
       "x4   0.007280  0.015172  0.017427  1.000000  0.018564 -0.114997 -0.017722   \n",
       "x5  -0.007541  0.032477  0.012066  0.018564  1.000000  0.063668  0.051840   \n",
       "x6  -0.021245  0.050837  0.091463 -0.114997  0.063668  1.000000 -0.007378   \n",
       "x7   0.022977  0.106724 -0.083206 -0.017722  0.051840 -0.007378  1.000000   \n",
       "x8   0.179157 -0.075864 -0.024772  0.021163  0.095510 -0.056266 -0.024446   \n",
       "x9   0.126103 -0.042676 -0.038316  0.031964 -0.004134  0.100071  0.095141   \n",
       "x10 -0.013969 -0.093224 -0.018879  0.035033 -0.011656  0.024867 -0.001231   \n",
       "y    0.424590  0.446384  0.139539 -0.006333  0.163366  0.011195 -0.446722   \n",
       "\n",
       "           x8        x9       x10         y  \n",
       "x1   0.179157  0.126103 -0.013969  0.424590  \n",
       "x2  -0.075864 -0.042676 -0.093224  0.446384  \n",
       "x3  -0.024772 -0.038316 -0.018879  0.139539  \n",
       "x4   0.021163  0.031964  0.035033 -0.006333  \n",
       "x5   0.095510 -0.004134 -0.011656  0.163366  \n",
       "x6  -0.056266  0.100071  0.024867  0.011195  \n",
       "x7  -0.024446  0.095141 -0.001231 -0.446722  \n",
       "x8   1.000000 -0.027991  0.028240  0.592573  \n",
       "x9  -0.027991  1.000000  0.111288 -0.169282  \n",
       "x10  0.028240  0.111288  1.000000 -0.050284  \n",
       "y    0.592573 -0.169282 -0.050284  1.000000  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_matrix = df.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "227a36aa-69d0-4306-b123-a996fbed1d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1     0.424590\n",
      "x2     0.446384\n",
      "x3     0.139539\n",
      "x4    -0.006333\n",
      "x5     0.163366\n",
      "x6     0.011195\n",
      "x7    -0.446722\n",
      "x8     0.592573\n",
      "x9    -0.169282\n",
      "x10   -0.050284\n",
      "y      1.000000\n",
      "Name: y, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# check the correlation between features and 'y'\n",
    "print(correlation_matrix.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296288f0-8c58-42d6-9b58-7a1de695ca61",
   "metadata": {},
   "source": [
    "Correlation matrix shows there is a linear relationship between the features and the dependent variable y. Some features show strong (x8) or moderate (x1, x2) linear relationships, while others show weak relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91dd4b5b-ca37-4029-8830-093bd8f2b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"y\", axis = 1)\n",
    "y = df[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "88d66218-d587-489b-8047-def3b8a4145a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 24.04775135367857\n",
      "R^2 = 0.9993257739212623\n",
      "coefficients = [ 7.16495583  7.76348129  4.52303273 -0.99797014  7.21681612  0.35211877\n",
      " -5.78016538  5.25655544 -1.26818131  0.66884165]\n",
      "intercept = 11.903160389190305\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error = {mse}\")\n",
    "print(f\"R^2 = {r2}\")\n",
    "\n",
    "print(f\"coefficients = {model.coef_}\")\n",
    "print(f\"intercept = {model.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62662b63-ef37-4836-8535-bc5408c31f06",
   "metadata": {},
   "source": [
    "MSE (Mean Squared Error) = 24.047751353678542 is relatively high, which may mean that the model is not perfect. \\\n",
    "R squared is used to assess the quality of linear regression models. It measures how well the predicted values ​​from the model match the actual values (my value is close to 1 (0.9993) , so there is almost no difference between the actual and predicted values.) \\\n",
    "Coefficients represent the influence of each independent variable on the dependent variable. \\\n",
    "The intercept is the value of y when all independent variables are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5f9a49-cee6-4537-b82f-d8148a45c0c2",
   "metadata": {},
   "source": [
    "### Problem 3. Figure out the modelling function (1 point)\n",
    "The modelling function for linear regression is of the form\n",
    "$$ \\tilde{y} = \\sum_{i=1}^{m}a_i x_i + b $$\n",
    "\n",
    "If you want to be clever, you can find a way to represent $b$ in the same way as the other coefficients.\n",
    "\n",
    "Write a Python function which accepts coefficients and data, and ensure (test) it works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0800a26d-3204-49ad-8a83-37edf0f4128f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients including intercept: [ 7.16495583  7.76348129  4.52303273 -0.99797014  7.21681612  0.35211877\n",
      " -5.78016538  5.25655544 -1.26818131  0.66884165  0.        ]\n",
      "Intercept: 11.903160389190589\n"
     ]
    }
   ],
   "source": [
    "# add a column of '1's to X to include the intercept\n",
    "X_with_intercept = np.hstack((X, np.ones((X.shape[0], 1))))\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_with_intercept, y)\n",
    "\n",
    "coefficients = model.coef_\n",
    "intercept = model.intercept_\n",
    "\n",
    "print(f\"Coefficients including intercept: {coefficients}\")\n",
    "print(f\"Intercept: {intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629ec102-4cdf-4624-ac10-0e958e800134",
   "metadata": {},
   "source": [
    "The last coefficient (0.0) corresponds to the intercept, which is included to be represented as part of the same coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dd3c8b95-8ac0-4160-9c65-6b6e7443fc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 661.55545456  448.86411728  176.87222935    9.32188657  108.35101666\n",
      "  293.2387376   352.02344265  304.63437918  266.57120492  212.47622852\n",
      "  297.17961156   51.09777779  282.13841949  250.06645408  557.32591963\n",
      "  437.55305299  293.22188151  224.39175571  423.62350286  522.75519081\n",
      "  185.65632002  564.13806958   81.45449811  512.97775241  436.08539959\n",
      "  303.29719143  437.1561283   430.15485952  324.93710697  346.01993231\n",
      "  422.2491919   401.5181546   543.26456256  273.40052442  126.11229114\n",
      "  314.89134928  592.77778328   94.43390407  110.37318181  313.18134777\n",
      "  534.56779678  477.80774987  300.34234332  298.84002912  514.06505813\n",
      "  343.68941891  635.36823915  512.58928891  605.00761879  281.87605893\n",
      "  436.11926634  330.94301185  282.34503917  276.1774465    75.10889022\n",
      "  216.81005089  504.67754771  102.4760338   554.78699167  699.75222004\n",
      "  414.43832543  392.78654929   -4.55656214  464.59205523  368.38421998\n",
      "  459.5812385   204.38120681  137.39795878  460.88080431  419.38310541\n",
      "  448.27894872  330.44978104  -27.04710626  552.14097625  767.65405522\n",
      "   49.56937914  319.69844897  458.06055926  160.07600003  365.35767425\n",
      "  420.8194305   154.91417615  446.55519108  365.62955637  387.2476721\n",
      "  434.28068639  424.46021122  336.53166552 -171.53177383  365.38724034\n",
      "  233.43746529  447.16543711  363.25224269  370.33619013  -81.88785208\n",
      "  475.80831609  766.14621678  259.69592311   58.05388538  153.53138456\n",
      "  414.90488684  450.7345881   289.48562017  304.13353738  145.31305167\n",
      "  249.60712176  374.49688697  516.78198555  456.03444791  556.076726\n",
      "  416.50266303  394.51974796  441.62205133  123.62184749  574.24349871\n",
      "  175.60292136  301.50875385  392.29563631 -104.21628812  149.15612804\n",
      "  142.85302115  108.70222985  564.09817422  299.33415171  295.95618169\n",
      "  -39.52068421  137.98655545  345.56563814  495.99624724  713.82794433\n",
      "  364.11073542  364.85190753  206.36107635  412.45692097  288.03394524\n",
      "  326.67855887   27.86803187  260.26149475  150.42415065   32.87473419\n",
      "  327.96134976  175.87938997  301.39246965  425.30989715  124.9953149\n",
      "  484.48857346  459.19028847  145.46137545  385.61889027  404.7979091\n",
      "  383.6047168   376.6137084   -32.057662    306.15768436  230.83860281\n",
      "  206.98070834  325.25678433  682.89042858  399.02045719  488.4213232\n",
      "  201.06752063  402.33735508   42.69463368  224.43190965   35.46189113\n",
      "  340.77007058  214.56444044  280.58958379  389.04659455   39.01941399\n",
      "  275.51044919  643.98409923  418.09395243  224.04601093  556.14395522\n",
      "  417.81154387  404.88591647  389.93794151  376.18075971  143.61304528\n",
      "  349.81254082  138.3167018   308.24003558  731.64940097  735.32501193\n",
      "  588.92714188  253.45192376 -180.1563343   475.20382479  279.48540665\n",
      "  171.30644592  406.18551121  381.71641488  296.94979688  376.43510649\n",
      "  195.97394867  530.80234357 -177.13425316  384.79480334  -57.69500146\n",
      "  614.86613683  307.62977693  383.1043634    96.43124258  454.01670735\n",
      "  852.44208861  439.79544157  359.30126479  305.36054716  283.62387103\n",
      "  -25.22727385  120.54306361  179.0723413   509.25489272  439.05499344\n",
      "    3.6207559   367.98060508  377.66856883  455.31080166  369.03459028\n",
      "  434.9661918   309.84209324  198.69206065  387.13168065  377.16376599\n",
      "  467.57557215  453.33032208  390.92302332  576.49116211  165.38438505\n",
      "  491.78173273  403.46790792  412.28906172  786.67682107  319.40431027\n",
      "  225.33661501  554.9847008   477.55283355  310.0113021   305.23148331\n",
      "  278.1762271   485.68136482  219.40302754  187.61236574  657.03892295\n",
      "  481.05137282 -150.50669854  293.47356752  724.10368837  217.36319327\n",
      "  229.73413282  813.1214923   332.91118707  219.54745707  460.63373036\n",
      "  473.1914822   575.54891969  683.11896757  244.19995424  325.49864397\n",
      "   66.02167174   90.21789238   58.63507473  138.48826009  226.07406034\n",
      "  413.07224338  -33.97182551  340.92571417   98.65718875  403.56356819\n",
      "  223.18508435   91.26241787  420.75747864  205.17328926  357.25119315\n",
      "  309.64897856  339.35983121  383.52551377  198.69186545  250.62687714\n",
      "  447.13952835  559.89495763  266.87126428  857.96151549  179.01028421\n",
      "  298.05711143   54.02397845    9.27527512  265.67041096  379.05852978\n",
      "  156.1286448   335.5915561   313.96943461  -72.00851861  229.86815896\n",
      "  289.02985434  -81.01332733  287.16905504  243.2953492   524.57397152]\n",
      "Mean Squared Error: 165.73297860447482\n",
      "R^2 = 0.9953533494820956\n"
     ]
    }
   ],
   "source": [
    "def model_function(coefficients, X):\n",
    "    X_with_intercept = np.hstack((X, np.ones((X.shape[0], 1))))\n",
    "    y_pred = np.dot(X_with_intercept, coefficients)\n",
    "    \n",
    "    return y_pred\n",
    "    \n",
    "y_pred = model_function(coefficients, X)\n",
    "print(y_pred)\n",
    "print(f\"Mean Squared Error: {mean_squared_error(y, y_pred)}\")\n",
    "print(f\"R^2 = {r2_score(y, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9d650-0994-4ae8-bba9-3b8fdecbdd3e",
   "metadata": {},
   "source": [
    "### Problem 4. Write the cost function and compute its gradients (1 point)\n",
    "Use MSE as the cost function $J$. Find a way to compute, calculate, or derive its gradients w.r.t. the model parameters $a_1, ..., a_m, b$\n",
    "\n",
    "Note that computing the cost function value and its gradients are two separate operations. Quick reminder: use vectorization to compute all gradients (maybe with the exception of $\\frac{\\partial J}{\\partial b}$) at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "99388ae1-1b23-4233-acf9-daf589bba07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the mean squared error (MSE) between the actual values ​​and the predicted values:\n",
    "def compute_cost(X, y, coefficients, intercept):\n",
    "    predictions = X @ coefficients + intercept\n",
    "    mse = np.mean((y - predictions) ** 2)\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0bcab0e3-66ec-4ac0-9124-310f14103681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the gradients of the cost function about the coefficients and the intercept:\n",
    "def compute_gradients(X, y, coefficients, intercept):\n",
    "    predictions = X @ coefficients + intercept\n",
    "    gradients = -2 / len(y) * (X.T @ (y - predictions))\n",
    "    intercept_gradient = -2 / len(y) * np.sum(y - predictions)\n",
    "    \n",
    "    return np.append(gradients, intercept_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "39244971-9382-4cba-adc6-52553c5312c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 24.047751353678542\n",
      "Gradients including intercept: [ 3.41591052e-12  3.81229862e-12  3.95251239e-13  7.18121858e-13\n",
      "  2.83459182e-13  6.54836185e-13 -4.66873947e-13  7.23048288e-13\n",
      "  3.94417536e-12  2.66027200e-13  7.36832817e-14]\n"
     ]
    }
   ],
   "source": [
    "# use generated data from previous tasks:\n",
    "df = pd.read_csv(\"linear_regr_data.csv\")\n",
    "X = df.drop(\"y\", axis = 1).values\n",
    "y = df.y.values\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "coefficients = model.coef_\n",
    "intercept = model.intercept_\n",
    "\n",
    "mse = compute_cost(X, y, coefficients, intercept)\n",
    "gradients = compute_gradients(X, y, coefficients, intercept)\n",
    "\n",
    "print(f\"MSE = {mse}\")\n",
    "print(f\"Gradients including intercept: {gradients}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bad786e-4299-4900-ae86-16cbd2235a6e",
   "metadata": {},
   "source": [
    "### Problem 5. Perform gradient descent (1 point)\n",
    "Perform weight updates iteratively. Find a useful criterion for stopping. For most cases, just using a fixed (large) number of steps is enough.\n",
    "\n",
    "You'll need to set a starting point (think about which one should be good, and how it matters); and a learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280d1e33-230f-4f83-a721-1c47c505abfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d18a168f-de4c-4035-88db-7350c2b91745",
   "metadata": {},
   "source": [
    "### Problem 6. Do other cost functions work? (2 points)\n",
    "Repeat the process in problems 4 and 5 with MAE, and then again - with the [Huber loss](https://en.wikipedia.org/wiki/Huber_loss). Both of them are less sensitive to outliers / anomalies than MSE); with the Huber loss function being specifically made for datasets with outliers.\n",
    "\n",
    "Explain your findings. Is there a cost function that works much better? How about speed of training (measured in wall time)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f0eb74-7b3d-4aa8-9749-dcb55e918a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d4176d1-6cad-4830-824c-1d27e50efe89",
   "metadata": {},
   "source": [
    "### Problem 7. Experiment with the learning rate (1 point)\n",
    "Use your favorite cost function. Run several \"experiments\" with different learning rates. Try really small, and really large values. Observe and document your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f04525-95f3-427d-8763-27af6839cb93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91385302-2fc1-48a4-a453-cce7f038e9c2",
   "metadata": {},
   "source": [
    "### Problem 8. Generate some data for classification (1 point)\n",
    "You'll need to create two clusters of points (one cluster for each class). I recomment using `scikit-learn`'s `make_blobs()` ([info](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)). Use as many features as you used in problem 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b948009d-6af6-468c-ac6e-bacd4f75498c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6246890-0808-40e1-b892-fa1d11fd088c",
   "metadata": {},
   "source": [
    "### Problem 9. Perform logistic regression (1 point)\n",
    "Reuse the code you wrote in problems 3-7 as much as possible. If you wrote vectorized functions with variable parameters - you should find this easy. If not - it's not too late to go back and refactor your code.\n",
    "\n",
    "The modelling function for logistic regression is\n",
    "$$ \\tilde{y} = \\frac{1}{1+\\exp{(-\\sum_{i=1}^{m}a_i x_i + b)}}$$. Find a way to represent it using as much of your previous code as you can.\n",
    "\n",
    "The most commonly used loss function is the [cross-entropy](https://en.wikipedia.org/wiki/Cross-entropy).\n",
    "\n",
    "Experiment with different learning rates, basically repeating what you did in problem 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e100b-1627-4300-8d2e-98e8b438e046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "411df95f-5f16-4c37-8649-2c495eb96974",
   "metadata": {},
   "source": [
    "### * Problem 10. Continue experimenting and delving deep into ML\n",
    "You just saw how modelling works and how to implement some code. Some of the things you can think about (and I recommend you pause and ponder on some of them are):\n",
    "* Code: OOP can be your friend sometimes. `scikit-learn`'s models have `fit()`, `predict()` and `score()` methods.\n",
    "* Data: What approaches work on non-generated data?\n",
    "* Evaluation: How well do different models (and their \"settings\" - hyperparameters) actually work in practice? How do we evaluate a model in a meaningful way?\n",
    "* Optimization - maths: Look at what `optimizers` (or solvers) are used in `scikit-learn` and why. Many \"tricks\" revolve around making the algorithm converge (finish) in fewer iterations, or making it more numerically stable.\n",
    "* Optimization - code: Are there ways to make the code run fastr?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08101fb6-d53f-4dc9-8f37-d07478c9e220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
