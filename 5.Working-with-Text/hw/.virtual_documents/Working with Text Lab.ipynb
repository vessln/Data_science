%matplotlib inline


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import string
import re
from collections import Counter

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

from sklearn.feature_extraction.text import CountVectorizer

from transformers import AutoTokenizer, TFDistilBertForSequenceClassification, pipeline
from sklearn.metrics import accuracy_score











restaurants_data = pd.read_csv("data/European Restaurant Reviews.csv")


restaurants_data.head(3)


restaurants_data.shape


print(f"There are {restaurants_data.shape[0]} observations.")


restaurants_data.dtypes


restaurants_data.info()


# make column names snake_case for easier work with the data:
restaurants_data.columns = restaurants_data.columns.to_series().apply(lambda x: x.replace(" ", "_").lower())
restaurants_data.columns


restaurants_data.review_date


# convert review date column to datetime type:
restaurants_data.review_date = restaurants_data.review_date.apply(lambda x: x.replace(" â€¢", ""))
restaurants_data.review_date = pd.to_datetime(restaurants_data.review_date, format = "%b %Y", errors = "coerce")
restaurants_data.review_date


most_repr_country = restaurants_data.country.value_counts().idxmax()
country_count = restaurants_data.country.value_counts().max()
print(f"The most represented country is {most_repr_country}. It occurs {country_count} times.")


max_date = restaurants_data.review_date.max().strftime("%B %Y")


min_date = restaurants_data.review_date.min().strftime("%B %Y")


print(f"Time range of dataset: from {min_date} to {max_date}.")


rest_review_counts = restaurants_data.restaurant_name.value_counts()
rest_review_counts


# check if all review counts are the same number:
is_balanced = rest_review_counts.nunique() == 1
print(f"Is the sample balanced in terms of restaurants - {is_balanced}.")


rest_sentiment_counts = restaurants_data.sentiment.value_counts()
rest_sentiment_counts


# check if all review counts are the same number:
is_balanced_sent = rest_sentiment_counts.nunique() == 1
print(f"Is the sample balanced in terms of sentiment - {is_balanced_sent}.")





restaurants_data.review


# make new columns 'review_words' with words from reviews:
restaurants_data["review_words"] = restaurants_data.review.str.split("\s+") # split words by one or more spaces
restaurants_data["review_words"] = restaurants_data.review_words.apply(lambda review_words: [w.lower() for w in review_words]) # make every word lowercase


# make new column 'review_words_count' with number of words in each review:
restaurants_data["review_words_count"] = restaurants_data.review_words.apply(lambda x: len(x))


restaurants_data.head()





positive_reviews_mean = restaurants_data.review_words_count[restaurants_data.sentiment == "Positive"].mean()
negative_reviews_mean = restaurants_data.review_words_count[restaurants_data.sentiment == "Negative"].mean()

positive_reviews_median = restaurants_data.review_words_count[restaurants_data.sentiment == "Positive"].median()
negative_reviews_median = restaurants_data.review_words_count[restaurants_data.sentiment == "Negative"].median()

positive_reviews_std = restaurants_data.review_words_count[restaurants_data.sentiment == "Positive"].std()
negative_reviews_std = restaurants_data.review_words_count[restaurants_data.sentiment == "Negative"].std()


print(f"Information about number of words in Negative reviews: " 
    f"mean = {negative_reviews_mean}, median = {negative_reviews_median}, std = {negative_reviews_std}.")

print(f"Information about number of words in Positive reviews: " 
    f"mean = {positive_reviews_mean}, median = {positive_reviews_median}, std = {positive_reviews_std}.")











# download NLTK data to remove unnecessary symbols and words:
nltk.download("stopwords") # for removing common words
nltk.download("punkt") # for removing punctuation
nltk.download("wordnet")


lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words("english"))

def text_preprocessing(review_words):
    review_words = ' '.join(review_words)
    review_words = review_words.translate(str.maketrans('', '', string.punctuation)) # remove punctuation
    
    tokens = word_tokenize(review_words) # tokenize the words
    
    tokens = [word for word in tokens if word.isalpha() and word not in stop_words] # remove everything (including stopwords) except alphabet

    lematized_tokens = ' '.join([lemmatizer.lemmatize(word) for word in tokens]) # lematize the tokens
    
    return lematized_tokens


restaurants_data["lematized_review_words"] = restaurants_data.review_words.apply(text_preprocessing)
restaurants_data["lematized_review_words"]


restaurants_data.head(3)





def return_top_words(lematized_words, n):
    words = " ".join(lematized_words).split() # make list of words
    words_count = Counter(words)
    
    return words_count.most_common(n)


words_from_positive_reviews = restaurants_data[restaurants_data.sentiment == "Positive"].lematized_review_words
words_from_negative_reviews = restaurants_data[restaurants_data.sentiment == "Negative"].lematized_review_words


n = 10
top_positive_words = return_top_words(words_from_positive_reviews, n)
top_negative_words = return_top_words(words_from_negative_reviews, n)

print(f"The top 10 words in positive reviews are:  {top_positive_words}\n")
print(f"The top 10 words in negative reviews are:  {top_negative_words}")








restaurants_data["title_words"] = restaurants_data.review_title.str.split("\s+")
restaurants_data["title_words"] = restaurants_data.title_words.apply(lambda title_words: [w.lower() for w in title_words]) 


restaurants_data["lematized_title_words"] = restaurants_data.title_words.apply(text_preprocessing)
restaurants_data = restaurants_data.drop(columns = "title_words")
restaurants_data.lematized_title_words


title_words_from_positive_reviews = restaurants_data[restaurants_data.sentiment == "Positive"].lematized_title_words
title_words_from_negative_reviews = restaurants_data[restaurants_data.sentiment == "Negative"].lematized_title_words


n = 10
top_positive_title_words = return_top_words(title_words_from_positive_reviews, n)
top_negative_title_words = return_top_words(title_words_from_negative_reviews, n)

print(f"The top 10 words in positive titles are:  {top_positive_title_words}\n")
print(f"The top 10 words in negative titles are:  {top_negative_title_words}")


def extract_words(words_frequencies):
    words_list = [word for word, f in words_frequencies]
    return words_list

set_top_positive_words_review = set(extract_words(top_positive_words))
set_top_negative_words_review = set(extract_words(top_negative_words))

set_top_positive_words_title = set(extract_words(top_positive_title_words))
set_top_negative_words_title = set(extract_words(top_negative_title_words))


positive_matching_words = set_top_positive_words_title.intersection(set_top_positive_words_review)
negative_matching_words = set_top_negative_words_title.intersection(set_top_negative_words_review)

print(f"The most frequently used words that occur in both positive reviews and titles: {', '.join(positive_matching_words)}.")
print(f"The most frequently used words that occur in both negative reviews and titles: {', '.join(negative_matching_words)}.")





restaurants_data.lematized_review_words = restaurants_data.lematized_review_words.str.split("\s+")
restaurants_data.lematized_title_words = restaurants_data.lematized_title_words.str.split("\s+")


top_positive_words = ['food', 'great', 'service', 'good', 'restaurant', 'place', 'nice', 'wine', 'menu', 'staff']
top_negative_words = ['restaurant', 'food', 'u', 'wine', 'table', 'menu', 'good', 'service', 'one', 'would']
top_positive_words_set = set(top_positive_words)
top_negative_words_set = set(top_negative_words)





def extract_positive_words(row):
    if row["sentiment"] == "Positive":
        review_words_set = set(row["lematized_review_words"])
        common_words = review_words_set.intersection(top_positive_words_set)
        return list(common_words)
    return np.nan 

def extract_negative_words(row):
    if row["sentiment"] == "Negative":
        review_words_set = set(row["lematized_review_words"])
        common_words = review_words_set.intersection(top_negative_words_set)
        return list(common_words)
    return np.nan 

restaurants_data["existing_top_pos_words_review"] = restaurants_data.apply(extract_positive_words, axis = 1)
restaurants_data["existing_top_neg_words_review"] = restaurants_data.apply(extract_negative_words, axis = 1)





top_positive_words_title = ['great', 'food', 'good', 'place', 'service', 'excellent', 'best', 'restaurant', 'dinner', 'amazing']
top_negative_words_title = ['food', 'service', 'disappointing', 'place', 'bad', 'rome', 'great', 'terrible', 'restaurant', 'ad']
top_positive_words_set_title = set(top_positive_words_title)
top_negative_words_set_title = set(top_negative_words_title)

def extract_positive_words(row):
    if row["sentiment"] == "Positive":
        review_words_set = set(row["lematized_title_words"])
        common_words = review_words_set.intersection(top_positive_words_set_title)
        return list(common_words)
    return np.nan 

def extract_negative_words(row):
    if row["sentiment"] == "Negative":
        review_words_set = set(row["lematized_title_words"])
        common_words = review_words_set.intersection(top_negative_words_set_title)
        return list(common_words)
    return np.nan 

restaurants_data["existing_top_pos_words_title"] = restaurants_data.apply(extract_positive_words, axis = 1)
restaurants_data["existing_top_neg_words_title"] = restaurants_data.apply(extract_negative_words, axis = 1)


restaurants_data.sample(3)


def has_common_words_for_sentiment(row, sentiment_type):
    if row["sentiment"] == sentiment_type:
        if sentiment_type == "Positive":
            list1 = row["existing_top_pos_words_review"]
            list2 = row["existing_top_pos_words_title"]
        elif sentiment_type == "Negative":
            list1 = row["existing_top_neg_words_review"]
            list2 = row["existing_top_neg_words_title"]
        else:
            return False
        return bool(set(list1) & set(list2))
    return False


restaurants_data["positive_correlation"] = restaurants_data.apply(
    lambda row: has_common_words_for_sentiment(row, "Positive"), axis=1)

restaurants_data["negative_correlation"] = restaurants_data.apply(
    lambda row: has_common_words_for_sentiment(row, "Negative"), axis=1)


total_positives = len(restaurants_data[restaurants_data.sentiment == "Positive"])
total_negatives = len(restaurants_data[restaurants_data.sentiment == "Negative"])

positive_with_top_words = len(restaurants_data[restaurants_data.positive_correlation == True])
negative_with_top_words = len(restaurants_data[restaurants_data.negative_correlation == True])

pct_positive_matching = (positive_with_top_words / total_positives) * 100
pct_negative_matching = (negative_with_top_words / total_negatives) * 100

print(f"{pct_positive_matching:.2f}% of all positive reviews contain one or more of the top positive words in both the review and the title.")
print(f"{pct_negative_matching:.2f}% of all negative reviews contain one or more of the top negative words in both the review and the title.")








restaurants_data.lematized_title_words.head(5)


restaurants_data.lematized_review_words.head(5)


# convert lematized words list to string:
restaurants_data["lematized_title_words_str"] = restaurants_data.lematized_title_words.apply(lambda x: " ".join(x))
restaurants_data["lematized_review_words_str"] = restaurants_data.lematized_review_words.apply(lambda x: " ".join(x))


restaurants_data.head(3)


# initialize vectorizers, set hyperparameters:

title_vectorizer = CountVectorizer(
    ngram_range = (1, 1),  # determines whether to use only single words (unigrams) or combinations of words
    stop_words = "english",
    lowercase = True,
    max_features = 1000,
    min_df = 2,             
    max_df = 0.90, 
)

review_vectorizer = CountVectorizer(
    ngram_range = (1, 2),   # capture unigrams and bigrams
    stop_words = "english", # remove common stop words
    lowercase = True,        # convert input words lowercase for consistency (ok in my case)
    max_features = 10000,   # take the first 10000 most common words
    min_df = 2,             # minimum number of times a word must occur
    max_df = 0.85,          # maximum number of times a word must occur
)


# fit and transform
title_features = title_vectorizer.fit_transform(restaurants_data.lematized_title_words_str)
review_features = review_vectorizer.fit_transform(restaurants_data.lematized_review_words_str)


title_features


review_features


pct_titles = (3662 / (1502 * 405)) * 100
pct_reviews = (54898 / (1502 * 7402)) * 100

print(f"Sparsity percentage for title matrix: {pct_titles:.2f}%")
print(f"Sparsity percentage for review content matrix: {pct_reviews:.2f}%")








# combine titles and reviews
restaurants_data["combined"] = restaurants_data["review_title"].fillna('') + ' ' + restaurants_data["review"].fillna('')

# load pre-trained model and tokenizer
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = TFDistilBertForSequenceClassification.from_pretrained(model_name)

# create a sentiment analysis pipeline
sentiment_pipeline = pipeline("sentiment-analysis", model = model, tokenizer = tokenizer, framework = 'tf')

# define the maximum length for the model
MAX_LENGTH = 512

# function to predict sentiment with truncation
def get_sentiment(text):
    encoded = tokenizer.encode(text, truncation = True, max_length = MAX_LENGTH, return_tensors = 'tf')
    result = sentiment_pipeline(encoded)[0]
    return 1 if result['label'] == 'POSITIVE' else 0

restaurants_data["predicted_sentiment"] = restaurants_data["combined"].apply(get_sentiment)

# Map true sentiment labels to integers
label_mapping = {'Positive': 1, 'Negative': 0}
restaurants_data['sentiment'] = restaurants_data['sentiment'].map(label_mapping)

# Evaluate accuracy
accuracy = accuracy_score(restaurants_data['sentiment'], restaurants_data['Predicted_Sentiment'])
print(f"Accuracy: {accuracy:.4f}")

# Display some results
print(restaurants_data[['Combined', 'Sentiment', 'Predicted_Sentiment']].head())













