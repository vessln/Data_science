%matplotlib inline


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os

from skimage.io import imread, imread_collection, imshow, imshow_collection
from skimage.transform import resize, rotate

from PIL import Image
import imagehash

import cv2

import tensorflow as tf
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions
from tensorflow.keras.models import Model

from sklearn.metrics import confusion_matrix











data_dir = "data/"
folders_cat_types = os.listdir(data_dir)

all_cat_types, image_count, image_sizes  = [], [], []

for cat_type in folders_cat_types:
    cat_type_path = os.path.join(data_dir, cat_type)
    
    if os.path.isdir(cat_type_path):
        image_files = [img for img in os.listdir(cat_type_path) if img.endswith(".jpg")]
        all_cat_types.append(cat_type)
        image_count.append(len(image_files))
        
        for image in image_files:
            image_path = os.path.join(cat_type_path, image)
            image = imread(image_path)
            image_sizes.append(image.shape)

df_cats = pd.DataFrame({"cat_type": all_cat_types, "image_count": image_count})

print(f"There are {len(all_cat_types)} (folders) types of cats.")
print(f"Total number of images: {df_cats.image_count.sum()}.")
df_cats


img_sizes_df_cats = pd.DataFrame(image_sizes, columns=["height", "width", "channel"])

mean_s = img_sizes_df_cats.mean()
mean_size = ", ".join([f"{round(el)}" for el in mean_s])
print(f"Typical image size: (height, width, channel) = ({mean_size})")

# identify outliers using threshold
size_threshold = mean_s + 2 * img_sizes_df_cats.std()
outliers = img_sizes_df_cats[
    (img_sizes_df_cats["height"] > size_threshold["height"]) | 
    (img_sizes_df_cats["width"] > size_threshold["width"])
]

print(f"There are {len(outliers)} outliers.")
print(outliers)


plt.figure(figsize = (8, 5))
plt.scatter(img_sizes_df_cats["width"], img_sizes_df_cats["height"], alpha = 0.7)
plt.xlabel("width")
plt.ylabel("height")
plt.title("Distribution of image sizes")
plt.grid()
plt.show()


all_types_cats = imread_collection("data/*") # read all folders from 'data' directory
african_wildcats_list = list(imread_collection("data/african-wildcat/*"))
blackfoot_cats_list = list(imread_collection("data/blackfoot-cat/*"))
chinese_cats_list = list(imread_collection("data/chinese-mountain-cat/*"))
domestic_cats_list = list(imread_collection("data/domestic-cat/*"))
european_wildcats_list = list(imread_collection("data/european-wildcat/*"))
jungle_cats_list = list(imread_collection("data/jungle-cat/*"))
sand_cats_list = list(imread_collection("data/sand-cat/*"))


all_folders = [african_wildcats_list, blackfoot_cats_list, chinese_cats_list, 
               domestic_cats_list, european_wildcats_list, junglke_cats_list, sand_cats_list]


all_images_sizes = np.array([])

for folder in all_folders:
    for image in folder:
        all_images_sizes = np.append(all_images_sizes, image.nbytes)
typical_image_size_bytes = round(all_images_sizes.mean())
typical_image_size_bytes


plt.hist(typical_image_size_bytes, bins = 50, color="r")

plt.xlabel("image size in bytes")
plt.ylabel("frequency")
plt.yticks(range(0, 550, 50))

plt.title("Distribution of image sizes")

plt.show()





def get_image_hash(image_path):
    with Image.open(image_path) as img:
        return imagehash.phash(img)

def find_and_remove_duplicates(dataset_path):
    image_hashes = {}
    duplicates = []

    for folder in os.listdir(dataset_path):
        folder_path = os.path.join(dataset_path, folder)
        
        if os.path.isdir(folder_path):
            for image_file in os.listdir(folder_path):
                if image_file.endswith(".jpg"):
                    image_path = os.path.join(folder_path, image_file)
                    image_hash = get_image_hash(image_path)
                    
                    if image_hash is not None:
                        if image_hash in image_hashes:
                            duplicates.append(image_path)
                        else:
                            image_hashes[image_hash] = image_path

    # print("\nDuplicate images:")
    # for img in duplicates:
    #     print(img)
    #     os.remove(img)

find_and_remove_duplicates(data_dir)





model = ResNet50()


model.summary()
print(f"Total number of layers: {len(model.layers)}")
print(f"Total number of parameters: {model.count_params():_}")





cat_image1 = imread("data/african-wildcat/af (11).jpg")
cat_image2 = imread("data/chinese-mountain-cat/ch (15).jpg")
cat_image3 = imread("data/domestic-cat/dc (63).jpg")
cat_image4 = imread("data/european-wildcat/eu (2).jpg")
cat_image5 = imread("data/sand-cat/sd (55).jpg")

cat_image1.shape, cat_image2.shape, cat_image3.shape, cat_image4.shape, cat_image5.shape














dataset_path = "data/"
output_dir_test = "test_preprocessed_img"
required_image_size = (224, 224)
batch_size = 4


def process_batches(dataset_path, batch_size, output_dir_test):
    if not os.path.exists(output_dir_test):
        os.makedirs(output_dir_test)
    
    batch_count = 0
    images_in_batch = []

    for folder in os.listdir(dataset_path):
        folder_path = os.path.join(dataset_path, folder)
        
        if os.path.isdir(folder_path):
            for image_file in os.listdir(folder_path):
                if image_file.endswith(".jpg"):
                    image_path = os.path.join(folder_path, image_file)
                    image = cv2.imread(image_path)
                    
                    if image is not None:
                        resized_image = cv2.resize(image, required_image_size)
                        preprocessed_image = preprocess_input(resized_image)
                        images_in_batch.append(preprocessed_image)
                        
                        if len(images_in_batch) >= batch_size:
                            batch_filename = f"batch_file_{batch_count}.npy"
                            np.save(os.path.join(output_dir_test, batch_filename), np.array(images_in_batch))
                            batch_count += 1
                            images_in_batch = []
    
    if images_in_batch:
        batch_filename = f"batch_file_{batch_count}.npy"
        np.save(os.path.join(output_dir_test, batch_filename), np.array(images_in_batch))


process_batches(dataset_path, batch_size, output_dir_test)


def load_all_batches(output_dir_test):
    batches = []
    
    for el in sorted(os.listdir(output_dir_test)):
        if el.endswith(".npy"):
            batch_path = os.path.join(output_dir_test, el)
            batch_images = np.load(batch_path)
            batches.append(batch_images)
            
    return np.concatenate(batches, axis=0)  # combine all batches


all_images = load_all_batches(output_dir)
print(f"Load all images: {all_images}")





model = ResNet50()
feature_extractor = tf.keras.Model(inputs=model.inputs, outputs=model.layers[-2].output)


# load all image batches
all_batches = []
for batch_id in range(len(os.listdir(PREPROCESSED_DIR))):
    batch_data = load_preprocessed_images(batch_id)
    all_batches.append(batch_data)

# flatten all_batches list into a single array
all_images_array = np.vstack(all_batches)

# extract features
features = feature_extractor.predict(all_images_array)
# make predictions
predictions = model.predict(all_images_array)

























